{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxGIAWgxwFEp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWOWlVhLwSt9"
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u0ia9-MwSvz",
    "outputId": "e4aca3ab-53ad-40b5-c714-1092621ac1b9"
   },
   "outputs": [],
   "source": [
    "# Checking the contents of the xrays folder\n",
    "data_dir = \"data/AugmentedAlzheimerDataset/\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyBGE4apwSxn",
    "outputId": "fb7fe525-c14b-44fd-dfe9-22d53ce8392f"
   },
   "outputs": [],
   "source": [
    "# Gettting the list of folders from data dir\n",
    "import os\n",
    "subfolders = os.listdir(data_dir)\n",
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pr0snsGMwSzf",
    "outputId": "b1df244e-c25e-4596-b319-213076757894"
   },
   "outputs": [],
   "source": [
    "# Getting list of img file paths (no folders)\n",
    "img_files = glob.glob(data_dir+\"**/*\")\n",
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4NxWWT_wS1R",
    "outputId": "b23e7708-ddd2-437f-9d1c-adec8b23ed8a"
   },
   "outputs": [],
   "source": [
    "# Take a look at the first 5 filepaths\n",
    "img_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmOJ8AI_wS3I",
    "outputId": "3d1a1479-5809-46e1-97e8-5b2fbd5403bd"
   },
   "outputs": [],
   "source": [
    "# Preview an example image (at full size)\n",
    "img_loaded = load_img(img_files[0])\n",
    "img_data = img_to_array(img_loaded)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5CWf3CxwS5C",
    "outputId": "12fe742c-b733-437a-fef1-40c86176c4a9"
   },
   "outputs": [],
   "source": [
    "# Data can be converted back to image\n",
    "array_to_img(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmNFMPz8xwUI"
   },
   "outputs": [],
   "source": [
    "# Saving image params as vars for reuse\n",
    "batch_size = 32\n",
    "img_height = 190\n",
    "img_width = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEUUwaJ7xwWD",
    "outputId": "1d3a6536-cf7e-4fe5-f576-3172b68019ee"
   },
   "outputs": [],
   "source": [
    "# make the dataset from the main folder of images\n",
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "    shuffle=True,\n",
    "    label_mode='categorical',\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAjDUpfTxwYG",
    "outputId": "2340572f-ddec-4727-bb21-097eb5fd67da"
   },
   "outputs": [],
   "source": [
    "# Determine number of batches in dataset\n",
    "ds_size = len(ds)\n",
    "ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDvaVGNyxwam",
    "outputId": "20f6181e-5994-4c6d-e0a1-b14591272463"
   },
   "outputs": [],
   "source": [
    "# taking a sample batch to see batch shape\n",
    "example_batch_imgs,example_batch_y= ds.take(1).get_single_element()\n",
    "example_batch_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jl0BYyMxwcc",
    "outputId": "0a1f3c41-0a25-47b1-a710-9b101e25d9ea"
   },
   "outputs": [],
   "source": [
    "# Preview y for first 5 of first batch\n",
    "example_batch_y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kbxsLagxweZ",
    "outputId": "15c7aaf1-453e-4572-9674-e978de1832d4"
   },
   "outputs": [],
   "source": [
    "# checking the class names\n",
    "class_names = ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_nQkl_0zAZD",
    "outputId": "7e1e847c-5a68-445e-ba03-466873ce874e"
   },
   "outputs": [],
   "source": [
    "# Saving # of classes for reuse\n",
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2hZ5Kibxwge",
    "outputId": "ced80bae-1f9f-4b6b-8eed-ce908b783a7d"
   },
   "outputs": [],
   "source": [
    "# Saving dictionary of integer:string labels\n",
    "class_dict = dict(zip(range(num_classes), class_names))\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuZfaRXay4w6",
    "outputId": "e1915baa-fcc5-4c21-ea36-e08b4c76793c"
   },
   "outputs": [],
   "source": [
    "# Individual image shape\n",
    "input_shape = example_batch_imgs[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTDU9ZS-y4zE",
    "outputId": "c593624a-9401-4e99-a6ea-e37a677bbcd2"
   },
   "outputs": [],
   "source": [
    "# Demo Unpacking shape\n",
    "input_shape = [*input_shape]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlkkQC3By464",
    "outputId": "a340e299-ca0e-437b-af0e-8ea383e94a74"
   },
   "outputs": [],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = 0.7\n",
    "split_val = 0.2\n",
    "split_test = .1\n",
    "# Calculate the number of batches for training and validation data\n",
    "n_train_batches =  int(ds_size * split_train)\n",
    "n_val_batches = int(ds_size * split_val)\n",
    "print(f\"Use {n_train_batches} batches as training data\")\n",
    "print(f\"Use {n_val_batches} batches as validation data\")\n",
    "print(f\"The remaining {len(ds)- (n_train_batches+n_val_batches)} batches will be used as test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_mqBepqy49M",
    "outputId": "53deb398-0858-4583-9770-67f9a317af5f"
   },
   "outputs": [],
   "source": [
    "# Use .take to slice out the number of batches\n",
    "train_ds = ds.take(n_train_batches)\n",
    "# Confirm the length of the training set\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_xeBCv5wS9m",
    "outputId": "7899fa16-a0f5-40ae-be34-fd8c803ea5f7"
   },
   "outputs": [],
   "source": [
    "# Skipover the training batches\n",
    "val_ds = ds.skip(n_train_batches)\n",
    "# Take the correct number of validation batches\n",
    "val_ds = val_ds.take(n_val_batches)\n",
    "# Confirm the length of the validation set\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah0V5CalzP8W",
    "outputId": "ed5f1d82-d279-44b9-ebf7-b0956a71b193"
   },
   "outputs": [],
   "source": [
    "# Skip over all of the training + validation batches\n",
    "test_ds = ds.skip(n_train_batches + n_val_batches)\n",
    "# Confirm the length of the testing data\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3qFrcVLzP-Q",
    "outputId": "368a5847-8efd-4b33-a932-f28e85edeb30"
   },
   "outputs": [],
   "source": [
    "# The original (non-take/non-skip) dataset contains the class_names\n",
    "class_names  = ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASjYXwGmzQAT"
   },
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(4, activation=\"softmax\")  # How many output possibilities we have\n",
    "    )  # What activation function are you using?\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuIDF4NZzQEf",
    "outputId": "bdb32cdd-034e-4c0b-b151-7648e1b62427"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model1 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osOj_x6KzQHD"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "# timing\n",
    "start = dt.datetime.now()\n",
    "\n",
    "# fit the neural network\n",
    "epochs=30\n",
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "end = dt.datetime.now()\n",
    "dur = end-start\n",
    "print(f\"Training time: {dur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8W_97J1zQOW"
   },
   "outputs": [],
   "source": [
    "# Use autotune to automatically determine best buffer sizes\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB-JiMuAzfMW",
    "outputId": "117bafa8-b1de-43d3-a6f9-7389a6680220"
   },
   "outputs": [],
   "source": [
    "# Make buffer size the same as the number of batches in train_ds\n",
    "buffer_size = len(train_ds)\n",
    "buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cO9bgIpLzfOe"
   },
   "outputs": [],
   "source": [
    "# Optimize training data\n",
    "train_ds = train_ds.cache().shuffle(buffer_size= buffer_size,\n",
    "                                   seed=42).prefetch(buffer_size=AUTOTUNE)\n",
    "# Optimize validation data\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# Optimize teset data\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gkw2QkczfQT",
    "outputId": "0e9a8b87-4bfe-494f-816d-f75f17df047a"
   },
   "outputs": [],
   "source": [
    "# Call build function to create identical model\n",
    "model2 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6V10MUMzfSf"
   },
   "outputs": [],
   "source": [
    "# See how long it takes to fit the optimized dataset\n",
    "# timing\n",
    "start = dt.datetime.now()\n",
    "# fit the neural network\n",
    "epochs=30\n",
    "history = model2.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    ")\n",
    "end = dt.datetime.now()\n",
    "dur2 = end-start\n",
    "print(f\"Training time: {dur2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRuq4fUQzfVD",
    "outputId": "95c075d3-1a99-47ec-8d64-b3e91c554bc4"
   },
   "outputs": [],
   "source": [
    "print(f\"The optimized dataset was {dur/dur2:.2f} times faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdCRl0BVzfWW"
   },
   "outputs": [],
   "source": [
    "model1.save('model1_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-1cfGJ2zfYU"
   },
   "outputs": [],
   "source": [
    "model2.save('model2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvH_DaEL1xaT"
   },
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "        # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "        # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(4, activation=\"softmax\")  # How many output possibilities we have\n",
    "    )  # What activation function are you using?\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwgvBbT_1xaU",
    "outputId": "ab2cc247-256d-4acb-ac5c-2f1b5627cac6"
   },
   "outputs": [],
   "source": [
    "# Call build function to create identical model\n",
    "model3 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU3rnktD1xaU",
    "outputId": "39cbc790-a865-4928-866a-e1bae790d8d0"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "# See how long it takes to fit the optimized dataset\n",
    "# timing\n",
    "start = dt.datetime.now()\n",
    "# fit the neural network\n",
    "epochs=10\n",
    "history = model3.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    ")\n",
    "end = dt.datetime.now()\n",
    "dur2 = end-start\n",
    "print(f\"Training time: {dur2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RZShki41xaU",
    "outputId": "0169f0f3-e236-4e7f-d34a-f5bf93f21507"
   },
   "outputs": [],
   "source": [
    "# Iterating through dataset object to separate image data from label data\n",
    "for images, labels in test_ds.as_numpy_iterator():\n",
    "    print(f'Image data shape is {images.shape}')\n",
    "    print(f'Label data shape is {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buIdh2wb1xaU",
    "outputId": "9b23a18a-dcad-4bf4-a6ff-80cef61657ae"
   },
   "outputs": [],
   "source": [
    "# Previewing a label (y_true)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHXtNu3e1xaX",
    "outputId": "afd41a55-a95f-4bce-f390-699167880b4d"
   },
   "outputs": [],
   "source": [
    "# Obtain predictions from images\n",
    "y_probs = model2.predict(images, batch_size=1)\n",
    "y_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwvN-fho1xaX"
   },
   "outputs": [],
   "source": [
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "\n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "\n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "\n",
    "    return y_true, y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2Dg1_BI1xaX",
    "outputId": "608e0d71-99b9-4964-f70d-67ec2abe3f75"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using the function\n",
    "y_test, y_pred_test = get_true_pred_labels(model2, test_ds)\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "print(f'Example of y_test: {y_test[0]}')\n",
    "print(f'Shape of y_pred_test {y_pred_test.shape}')\n",
    "print(f'Example of y_pred_test {y_pred_test[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX98dfI41xaX",
    "outputId": "c0beddb7-230d-48f8-fe55-f4998d46f505"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using the function\n",
    "y_test, y_pred_test = get_true_pred_labels(model3, test_ds)\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "print(f'Example of y_test: {y_test[0]}')\n",
    "print(f'Shape of y_pred_test {y_pred_test.shape}')\n",
    "print(f'Example of y_pred_test {y_pred_test[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoUQj9ve1xaX"
   },
   "outputs": [],
   "source": [
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "\n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")\n",
    "        return np.argmax(y, axis=1)\n",
    "\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrTEiowX1xaX",
    "outputId": "ff188df3-9b98-4cf2-f3e3-6eb947f22006"
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "y_preds_transformed = convert_y_to_sklearn_classes(y_pred_test, verbose = True)\n",
    "y_preds_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8H94wXw1xaX"
   },
   "outputs": [],
   "source": [
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "\n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "\n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "\n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None,\n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\",\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "\n",
    "\n",
    "    # Create a confusion matrix with the data with normalize argument\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap,\n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "\n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "\n",
    "\n",
    "\n",
    "## PLOT_HISTORY FUNCTION FROM WEEK 3\n",
    "def plot_history(history,figsize=(6,8)):\n",
    "    # Get a unique list of metrics\n",
    "    all_metrics = np.unique([k.replace('val_','') for k in history.history.keys()])\n",
    "\n",
    "    # Plot each metric\n",
    "    n_plots = len(all_metrics)\n",
    "    fig, axes = plt.subplots(nrows=n_plots, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through metric names add get an index for the axes\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "\n",
    "        # Get the epochs and metric values\n",
    "        epochs = history.epoch\n",
    "        score = history.history[metric]\n",
    "\n",
    "        # Plot the training results\n",
    "        axes[i].plot(epochs, score, label=metric, marker='.')\n",
    "        # Plot val results (if they exist)\n",
    "        try:\n",
    "            val_score = history.history[f\"val_{metric}\"]\n",
    "            axes[i].plot(epochs, val_score, label=f\"val_{metric}\",marker='.')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            axes[i].legend()\n",
    "            axes[i].set(title=metric, xlabel=\"Epoch\",ylabel=metric)\n",
    "\n",
    "    # Adjust subplots and show\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP36Fyzo1xaX"
   },
   "outputs": [],
   "source": [
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "\n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "\n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "\n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None,\n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\",\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "\n",
    "\n",
    "    # Create a confusion matrix with the data with normalize argument\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap,\n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "\n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "\n",
    "\n",
    "\n",
    "## PLOT_HISTORY FUNCTION FROM WEEK 3\n",
    "def plot_history(history,figsize=(6,8)):\n",
    "    # Get a unique list of metrics\n",
    "    all_metrics = np.unique([k.replace('val_','') for k in history.history.keys()])\n",
    "\n",
    "    # Plot each metric\n",
    "    n_plots = len(all_metrics)\n",
    "    fig, axes = plt.subplots(nrows=n_plots, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through metric names add get an index for the axes\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "\n",
    "        # Get the epochs and metric values\n",
    "        epochs = history.epoch\n",
    "        score = history.history[metric]\n",
    "\n",
    "        # Plot the training results\n",
    "        axes[i].plot(epochs, score, label=metric, marker='.')\n",
    "        # Plot val results (if they exist)\n",
    "        try:\n",
    "            val_score = history.history[f\"val_{metric}\"]\n",
    "            axes[i].plot(epochs, val_score, label=f\"val_{metric}\",marker='.')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            axes[i].legend()\n",
    "            axes[i].set(title=metric, xlabel=\"Epoch\",ylabel=metric)\n",
    "\n",
    "    # Adjust subplots and show\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUMXdyB01xaX"
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_network(model,\n",
    "                                    X_train=None, y_train=None,\n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\",\n",
    "                                    colorbar=False):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "\n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "\n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred,\n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data')\n",
    "\n",
    "        ## Run model.evaluate\n",
    "        print(\"\\n- Evaluating Training Data:\")\n",
    "        print(model.evaluate(X_train, return_dict=True))\n",
    "\n",
    "    # If no X_train, then save empty list for results_train\n",
    "    else:\n",
    "        results_train = []\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "\n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred,\n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data')\n",
    "\n",
    "        ## Run model.evaluate\n",
    "        print(\"\\n- Evaluating Test Data:\")\n",
    "        print(model.evaluate(X_test, return_dict=True))\n",
    "\n",
    "    # If no X_test, then save empty list for results_test\n",
    "    else:\n",
    "        results_test = []\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    if output_dict == True:\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ISbY-c81xaY"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model1, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2ej7v5o1xaY"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model1, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJm2RJHr1xaY"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model2, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOwnqNSP1xaY",
    "outputId": "ce58efc6-3c5a-43d8-e5b8-299ec5da1acf"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model3, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxo1BRHh1xaY",
    "outputId": "fbb2fe94-795a-4ac0-b1fa-284ef9421b7f"
   },
   "outputs": [],
   "source": [
    "model1.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOyZqR9B1xaY"
   },
   "outputs": [],
   "source": [
    "model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWQPenfE1xaY"
   },
   "outputs": [],
   "source": [
    "model3.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D923ypMx1xaY",
    "outputId": "759f41d6-a11a-4fc8-b8d8-3d21cf6183ab"
   },
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4OHeubr1xaY"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing import image\n",
    "# img = image.load_img('./data/xRays/normal/00000007_000.png',target_size=(96, 96))\n",
    "# img_array = image.img_to_array(img)\n",
    "# img_array = np.expand_dims(img_array, axis=0)\n",
    "# img_array /= 255.0  # Normalize the image\n",
    "# predictions = model2.predict(img_array)\n",
    "# print(predictions)\n",
    "# predicted_class = np.argmax(predictions[0])\n",
    "# confidence_percent = round((np.max(predictions[0]) * 100 ),3)\n",
    "# print(predicted_class)\n",
    "# print(class_names[predicted_class])\n",
    "# print(confidence_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMyUl0cV1xaY",
    "outputId": "13e0385a-c459-4643-e8c8-e486846eb193"
   },
   "outputs": [],
   "source": [
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMr5oYux1xaY"
   },
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "        # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "        # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(4, activation=\"softmax\")  # How many output possibilities we have\n",
    "    )  # What activation function are you using?\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8L6V_lq1xaY",
    "outputId": "52940cfe-a12c-4157-9cff-8936acfaa7fa"
   },
   "outputs": [],
   "source": [
    "print(len(train_ds))\n",
    "model5 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq5nwFOk1xaY",
    "outputId": "2dc45dd2-e419-4127-87e3-857be46a489e"
   },
   "outputs": [],
   "source": [
    "# See how long it takes to fit the optimized dataset\n",
    "# timing\n",
    "start = dt.datetime.now()\n",
    "# fit the neural network\n",
    "epochs=10\n",
    "history = model5.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    ")\n",
    "end = dt.datetime.now()\n",
    "dur2 = end-start\n",
    "print(f\"Training time: {dur2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THmIj8Fy1xaY",
    "outputId": "66853683-0f44-4017-850b-15f805e6fa29"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model5, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pXxR-0x1xaY",
    "outputId": "1b6a4654-eb91-40ed-cf0f-01ad645b7ace"
   },
   "outputs": [],
   "source": [
    "model5.save('model5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUtZj3lR1xaY"
   },
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    model.add(layers.Dropout(0.2))  # Dropout layer\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "        # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    model.add(layers.Dropout(0.2))  # Dropout layer\n",
    "        # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(4, activation=\"softmax\")  # How many output possibilities we have\n",
    "    )  # What activation function are you using?\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNuPMfv11xaY",
    "outputId": "a87848e3-9154-4e76-a3bd-b1adbb544615"
   },
   "outputs": [],
   "source": [
    "model6 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6O7jSZi1xaY",
    "outputId": "6827b6f1-0d5d-460d-960f-8baa21503c73"
   },
   "outputs": [],
   "source": [
    "# See how long it takes to fit the optimized dataset\n",
    "# timing\n",
    "start = dt.datetime.now()\n",
    "# fit the neural network\n",
    "epochs=10\n",
    "history = model6.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    ")\n",
    "end = dt.datetime.now()\n",
    "dur2 = end-start\n",
    "print(f\"Training time: {dur2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is1hezao1xaY",
    "outputId": "389ac353-2902-4620-9a90-a029bbbd235d"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model6, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVk0MwvO1xaY",
    "outputId": "dca915cb-bdd3-486c-8818-53723a144709"
   },
   "outputs": [],
   "source": [
    "model6.save('model6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iDy241_1xaZ"
   },
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    model.add(layers.Dense(500,kernel_regularizer=regularizers.l2(0.01),activation=\"relu\")) # R2 regularizer\n",
    "    model.add(layers.Dropout(0.2))  # Dropout layer\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    model.add(layers.Dense(500,kernel_regularizer=regularizers.l2(0.01),activation=\"relu\")) # R2 regularizer\n",
    "    model.add(layers.Dropout(0.2))  # Dropout layer\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=8,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        ))\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    model.add(layers.Dense(500,kernel_regularizer=regularizers.l2(0.01),activation=\"relu\")) # R2 regularizer\n",
    "    model.add(layers.Dropout(0.2))  # Dropout layer\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(4, activation=\"softmax\")  # How many output possibilities we have\n",
    "    )  # What activation function are you using?\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5hZwPVp1xaZ",
    "outputId": "5ce3bf2d-92df-4cff-c880-832a65569332"
   },
   "outputs": [],
   "source": [
    "model7 = build_model()\n",
    "# See how long it takes to fit the optimized dataset\n",
    "# timing\n",
    "start = dt.datetime.now()\n",
    "# fit the neural network\n",
    "epochs=10\n",
    "history = model7.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    ")\n",
    "end = dt.datetime.now()\n",
    "dur2 = end-start\n",
    "print(f\"Training time: {dur2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iFf8Zhi1xaZ"
   },
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model7, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mYzSOP31xaZ"
   },
   "outputs": [],
   "source": [
    "model7.save('model7_pt2.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
